---
title: "Overview of Transformer-based Pre-trained Language Models for Natural 
Language Processing"
collection: publications
permalink: /publication/Overview
excerpt: '**Tongyue Shi** and Zhongqing Wang'
date: 2022-4-4
venue: 'China Computer & Communication'
# paperurl: ''
# citation: '史童月,王中卿.基于Transformer的自然语言处理预训练语言模型概述[J].信息与电脑(理论版),2022,34(10):52-56.'
---
在自然语言处理领域，谷歌提出Transformer模型之后，以生成式预训练模型（Generative Pre-Training，GPT）和深度双向预训练语言模型（Bidirectional Encoder Representat ions from Transformers，BERT）等为代表的一些基于Transformer的预训练语言模型（Transformer-based Pre-trained Language Models，TPLM）相继被提出，在大部分自然语言处理任务上取得很好的效果。TPLM使用自监督学习方法学习来自大量文本数据的通用语言表示，并将这些知识转移到下游任务中，为其提供了背景知识，避免了重新开始训练新任务模型的情况。笔者主要研究了基于Transformer的预训练语言模型与基于TPLM的“预训练+微调”的自然语言处理预训练新技术模式。首先，介绍预训练模型的发展背景;其次，解释各种有关TPLM的核心概念，如预训练、预训练方法、预训练任务、自监督学习与迁移学习等;再次，简要介绍相关TPLM模型与目前的进展;最后，提出改进TPLM的方法并总结。 

[Download paper here](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDAUTO&filename=XXDL202210017&uniplatform=NZKPT&v=8Crfb2VCl6v_vYuaN2yBuVQpEE9lqWBmnbV-g-XG86QBWHxb0Uh-eXzOtvIDflg4)

Recommended citation: 史童月,王中卿.基于Transformer的自然语言处理预训练语言模型概述[J].信息与电脑(理论版),2022,34(10):52-56.